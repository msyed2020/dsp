Signal Statistics and Noise

-- What is a signal?

- A signal describes how one parameter RELATES to another
(signals are plotted on multiple axes to describe a relationship)

- Signals in the real world are naturally continuous
- It's when a continuous signal is passed thorugh an analog to
digital converter (ADC) that the signal becomes digitized, or discrete

-- Independent and Dependent Variables

- Signal is made up of two parameters, the independent variable and the
dependent variable
- Vertical axis represents the DEPENDENT variable
- Horizontal axis represents the INDEPENDENT variable
(independent - x-axis) (horizontal - y-axis)
- Dependent variable (y-axis) is a FUNCTION of the independent variable (x-axis)
- Functions usually tend to be y = x something lol
- Independent variable describes HOW AND WHEN a sample is taken
- Dependent variable is the ACTUAL measurement

-- Signal Samples

- Each digitized signal point is known as a SAMPLE
- The total # of samples is denoted by variable N

-- Mean and Standard Deviation

- Mean is the AVERAGE value of a signal
- It is found by adding all samples together, then
dividing the sum by total # of samples (N)
- Mean is denoted by mu (mu greek symbol from math lol)
- In electronics, the mean is also known as the DC value
- See figure1.jpg for formula

- Standard deviation is a measure of how far the signal
fluctuates from the mean
- The power of this fluctuation is known as variance
- Standard deviation is denoted by sigma greek symbol
- See figure2.jpg for formulas

- Variance is computed by squaring the standard deviation (sigma ^ 2)
- See figure3.jpg for formula


---

Sampling Theorem

-- Quantization

- Quantization is process of mapping continuous infinite values to a smaller set of discrete finite values
- Sample and Hold submodule converts independent variables (x-axis) from continuous to discrete
- Quantizer submodule converts dependent variables (y-axis) from continous to discrete
- Proper sampling is defined as the ability to reconstruct an exact analog signal from samples
- Sampling theorem states that a continuous signal can properly be sample ONLY IF it doesn't contain
frequency components above half the sampling rate
- Ex. if we sample at 50Hz (or 50 samples per second), the analog signal we are sampling MUST be made of
frequencies from 25Hz (25 cycles per second) and below
- This sampling theorem is also known as the Nyquist Theorem

- Before an analog signal enters the ADC, it is first passed through an analog filter
- This filter removes frequency components above half of the sampling rate. A filtered analog signal
is produced by this filter
- This is referred to as the anti-alias filter
- The filtered analog signal is passed through the ADC, and then a digitized signal is processed
- We can perform further digital processing on this signal if we wish
- If we wish to reconstruct our final analog signal, we can pass our output from our digital processing
through a DAC
- The output from the DAC is passed through another analog filter. This filter removes frequency components
above half the sampling rate
- This output filter is referred to as the reconstruction filter

-- Passive Filter

-- Passive Lowpass Filter

- This filter passes low frequencies and blocks high frequencies
- Constructed using only resistors and capacitors; it is also called the RC lowpass filter
- Waves from sensor pass by the Passive Lowpass Filter and go to the ADC
- The range of frequencies for which the filter causes a significant attenuation is
called the stopband
- On the contrary, the range of frequencies where the filter does NOT cause a significant attenuation (ranges other
than the stopband) is called the passband
- The cutoff frequency of an RC filter is the frequency in which the amplitude of the input signal is reduced
- This cutoff frequency is denoted by Fc
- At a high frequency, capacitor becomes a short circuit (a circuit connected to ground that doesn't exist)
- At a low frequency, capacitor becomes an open circuit
- Fc = 1 / 2*pi*RC (see figure4.jpg for formula)


-- Passive Highpass Filter

- This filter passes high frequencies and blocks low frequencies
- Constructed only using resistors and capacitors, hence considered RC
- In the highpass circuit, the resistor and capacitor location has swapped; resistor is connected to the ground
and capacitor is on the main circuit
- At a high frequency, the capacitor becomes a short circuit (turns into a wire and disappears)
- At a low frequency, the capacitor becomes an open circuit (broken/cut wire)
- Brick wall filter = ideal filter (a filter that doesn't exist, it has no 
transition between the passband and stopband)

-- Passive vs Active Filters

- Passive filters are made up of passive components only
- Passive components refer to resistors, capacitors, inductors
- Active filters, on the contrary, are made up of both active and passive components
- These include operational amplifiers (op amps) and transistors
- One of the advantages active filters have over passive filters is their ability to
provide signal gain (like to amplify the signal)
- See figure5.jpg for a photo of an Active Low Pass Filter
- See figure6.jpg for a photo of an Active High Pass Filter

- Most Common Configurations
  - Analog filters that are often used in real world applications are
  Chebyshev, Butterworth, and Bessel
  - Each of these filters is designed to optimize a different performance
  parameter
  - The complexity of each filter can be adjusted by selecting the number of
  poles and zeroes
  - The more poles in a filter, the more electronics it requires, and the better
  it performs

- Poles and Zeroes of a transfer function are frequencies for which the value of a
denominator and numerator of a transfer function become zero, respectively. The values
of poles and zeroes of a system determine whether or not the system is stable

- The Modified Sallen-Key filter serves as a building block for designing active filters
such as Chebyshev, Butterworth, and Bessel

- See figure7.jpg for a diagram of a low pass Modified Sallen-Key filter
- See figure8.jpg for Resistor formulas

-- Bessel, Chebyshev, and Butterworth Filters

- Roll-off: A drop in amplitude
- Chebyshev provides the sharpest roll-off, aka the fastest drop in amplitude, and characterized by ripples in the passband
- Butterworth provides the flattest passband amongst the three, and is optimized to provide the sharpest roll off possible
without allowing a ripple in the passband
- The Bessel filter has no ripple in the passband, but the roll off is far worse than the Butterworth
- The graphs for these filters can be viewed in figure9.jpg

- The step response tells us how the filter responds when the input rapidly changes from
one value to another
- Chebyshev step response is characterized by overshoot and oscillations that slowly decrease in amplitude
- Butterworth step response is characterized by overshoots and oscillations that also slowly decrease in
amplitude, but less than that of Chebyshev
- Bessel filter has no overshoots and oscillations. It is the best step response

- Chebyshev step response at 1 Hz fc can be viewed in figure10.jpg
- Butterworth step response at 1 Hz fc can be viewed in figure11.jpg
- Bessel step response at 1 Hz fc can be viewed in figure12.jpg

-- Linear Systems

- What is a system?
  - A system is a process that produces an output signal, in response
  to an input signal

- A signal describes how one parameter relates to another
- Continuous signals use parentheses, ex. x(t) or y(t)
- Discrete signals use brackets, ex. x[t] or y[t]
- Uppercase letters are used when working with frequency domains

- Information encoding in an analog waveform
  - Time domain encoding: Information is encoded in the sine waves of a signal
  - Frequency domain encoding: Information is encoded in the shape of the waveform

- Properties of a Linear System
  - Homogeneity
  - Additivity
  - Shift Invariance

- Homogeneity Property
  - A change in the amplitude of the input signal results in the corresponding
  change in the amplitude of an output signal
  - For instance x[n] going in a system should result in y[n], and
  k*x[n] going in a system should result in k*y[n]

- Additivity Property
  - A system is said to be additive if added signals pass through it without interacting
  - x1[n] going in the system should result in y1[n]
  - x2[n] going in the system should result in y2[n]
  - Therefore, x1[n] + x2[n] going in the system should result in y1[n] + y2[n] (no mixture)

- Shift Invariance Property
  - Shift in the input signal causes an identical shift in the output signal
  - x[n] going in the system should result in y[n]
  - x[n + s] going in the system should result in y[n + s]

-- Superposition

- Superposition is the response of a linear system to a sum of signals is the sum of responses to each individual
input signal

- Synthesis and Decomposition
  - Synthesis involves adding two or more signals, to form a resulting signal
  - Decomposition is the opposite of synthesis; it involves breaking one signal
  into two or more additive component signals

-- Impulse and Step Decomposition

- Impulse decomposition breaks a signal of N samples into N component signals,
each of which contain N samples
- Each of the component signals contains a point from the original signal, but the
rest of the values are zero
- A single nonzero point in a string of zeroes is called an Impulse
- Impulse decomposition allows signals to be examined, using one sample at a time
- By knowing how a system responds to an impulse, the systems' output can be calculated
for any given input
- Step decomposition also breaks an N samples signal into N component signals, each of
which contain N samples
- Each component signal is a step, which means that the first samples have a value of zero,
while the last samples have some constant value
- Step decomposition characterizes signals by the difference between adjacent samples


-- Convolution

- A mathematical operation of combining two signals together, then getting a resultant
signal
- Convolution relates 3 signals; the input signal, output signal, and impulse response
- The delta function is a normalized input response; the sample number zero has a value of
one, while all other samples have a value of zero
  - The delta function is also called the "unit impulse"
  - It is denoted by &[n], or "delta of n" (pretend the & is a delta lol)
- The impulse response is the signal produced as output when the delta function is provided
as input to the system
  - Two different systems produce two different impulse responses
  - The impulse response is denoted by h[n]
- For example, a delta function &[n] passed through a system has an output of
h[n] (the impulse response)
- Any impulse can be represented as a shifted/scaled delta function
- If we had a function a[n] where all values are 0, except n = 8 on the graph,
which is -3, we say that a[n] = -3&[n - 8]
- Same as the delta function shifted to the right by 8 samples, and
multiplied by -3

- In digital filter design, the impulse response is called the filter kernel,
the convolution kernel, or just the kernel
- In image processing, the image response is called the point spread function


-- The Convolution Operation 

- The convolution operation is denoted by *

- Input signal (x[n]) * inpulse signal (h[n]) = output signal (y[n])

- A 9 point input signal, x[n], is passed through a system with a 4 point
impulse response, h[n], which results in a 9 + 4 - 1 = 12 point output
signal, y[n].

  - Input signal is decomposed into components
  - Each component is passed through the system
  - Each of the 9 samples in the input signal will
  contribute a scaled and shifted version of the
  impulse response to the output signal
  - Output components produced by the system are
  synthesized to produce the output signal

- Commutative Property of Convolution: x[n] + h[n] = y[n]
is the same as h[n] + x[n] = y[n]

-- The Convolution Sum Equation

If x[n] is an N point signal that's running from 0 to N - 1, and h[n] is an M point signal running
from 0 to M - 1, then the convolution of the two is:

y[n] = h[n] * x[n]

y[n] is an N + M - 1 point signal running from 0 to N + M - 2, given by:

See figure13.jpg for the formula

This equation allows each point in the output signal to be calculated
INDEPENDENTLY of all other points in the output signal

The index i determines which sample in the output signal is being
calculated

The index j is an iterator running through the impulse response


-- Impulse Response Revisited

- Characteristics of a linear system is completely specified by
the system's impulse response

- The simplest impulse response is the delta function


-- First Difference/Running Sum

- First Difference equation: y[n] = x[n] - x[n - 1]

- The output signal at a given point is equal to the difference between two adjacent
signals in the input signal

- Running Sum equation: y[n] = x[n] + y[n - 1]

- The output signal at a given point is equal to the sum of the input signal
plus the previous value in the output signal

- These are also known as recursive signals and play a role in designing
infinite impulse responses


-- Discrete Fourier Transform

- A Fourier Transform is a set of mathematical techniques that are based
on decomposing signals into sinusoids

- Discrete Fourier Transform is the version of Fourier Transform for
discrete signals

- Any continuous periodic signal could be represented as the sum of
properly chosen sinusoidal waves

- Ex: Fourier decomposition of a 16 point long signal into 9 cosine waves
and 9 sine waves, each with a different amplitude and frequency
Combining these waves together will reproduce the original 16 point
long signal

- Categories of signals and Fourier Transforms:
A signal can be either continuous or discrete, and it can be either
periodic or aperiodic

- This leads to 4 categories of signals: Aperiodic-Continuous,
Periodic-Continuous, Aperiodic-Discrete, Periodic-Discrete

- Aperiodic-Continuous signals extend to both positive and negative
infinity without repeating a periodic pattern
  - Examples include the Gaussian curve and exponential decay curve
  - Fourier Transform applied to these signals are called Fourier
  Transform

- Periodic-Continous signals are signals that repeat themselves
in a regular pattern from negative to positive infinity
  - Examples include sine waves, square waves, etc
  - Fourier Transform applied to this type of signal is called Fourier
  Series

- Aperiodic-Discrete signals are only defined at discrete points
between positive and negative infinity, and don't repeat themselves
in a periodic pattern
  - Fourier Transform applied to the type of signal is called Discrete
  Time Fourier Transform (DTFT)

- Periodic-Discrete signals are signals that are discrete and repeat
themselves in a periodic pattern
  - Fourier Transform applied to the type of signal is called
  Discrete Fourier Transform (DFT)

- Each of the four Fourier transforms can be subdivided into real
and complex versions
- The real version uses ordinary numbers while the complex version
uses complex numbers

-- The DFT Engine

- See figure14.jpg for diagram of DFT Engine

- When an N-point time domain signal x[] is
running from 0 to N-1 is passed through DFT,
2 frequency domain signals are produced
- Each of these signals are N/2 + 1 points long,
and run from 0 to N/2

- The real signal is written as ReX[]
- The imaginary signal is written as ImX[]

-- Time Samples vs Frequency Domain

- Time domain refers to samples taken over time
- Frequency domain describes the amplitude of the sine
and cosine waves produced
- The frequency domain contains exactly the same information
as the time domain, but just in a different format
- Knowing one domain allows us to calculate for the other
- Time domain signals are represented by lower case letters,
example given, x[] or y[]
- Frequency domain signals are represented by uppercase letters,
example given, X[] or Y[]

- Time domains go through a DFT such as Forward DFT, DFT, analysis,
decomposition, etc to become a frequency domain
  - This is known as Forward DFT
- Frequency domains that use Inverse DFT or Synthesis become a time
domain, known as Inverse DFT
- Calculating time domain from frequency domain is known as Synthesis,
or Inverse DFT (IDFT)

- Horizontal axis of the frequency domain can be labelled 3 different
ways:
  - As an array index running from 0 to N/2
  - As a fraction of the sampling frequency running from 0 to 0.5
  - As a natural frequency running from 0 to pi


-- Polar Notation

- Rectangular Notation: ReX[k], ImX[k]
- Polar Notation: MagX[k], PhaseX[k]

- Acos(x) + Bsin(x) = Mcos(x + theta)
- M = sqrt(A^2 + B^2)

- theta = arctan(B / A)

- Rectangular to Polar Conversion:
  - MagX[k] = sqrt(ReX[k]^2 + ImX[k]^2)
  - PhaseX[k] = arctan(ImX[k] / ReX[k])

- Polar to Rectangular Conversion:
  - ReX[k] = MagX[k] * cos(PhaseX[k])
  - ImX[k] = MagX[k] * sin(PhaseX[k])

-- Complex Numbers

- Complex number example: 2 + 6j, -4 - 1.5j, 3 - 7j

- Separated into Real (Re) and Imaginary (Im) parts:
- Re: 2, Im: 6. Re: -4, Im: -1.5. Re: 3, Im: -7 (as per example above)

- Adding/subtracting/multiplying complex number equations require matching like terms,
ex. 3 + 5j + 4 + 2j = 7 + 7j

- Polar Notation of Complex Numbers

- M = sqrt((ReA)^2 + (ImA)^2)
- theta = arctan(ImA/ReA)
- ReA = Mcos(theta)
- ImA = Msin(theta)
- a + jb = M(cos(theta) + j sin(theta))

- Euler's Relation

- e^(jx) = cosx + jsinx
  - Can also be rewritten as: a + jb = Me^(j*theta)

- M1e^(j*theta1) * M2e^(j*theta2) = M1M2e^(j * (theta1 + theta2))

- M1e^(j*theta1) / M2e^(j*theta2) = (M1 / M2) * e^(j * (theta1 - theta2))

- Representation of Sinusoids

- A*cos(wx) + B*sin(wx) <=> a + jb, where w = 2*pi*f (f is natural frequency)

- A <=> a: Amplitude of cosine wave
- B <=> -b: Negative amplitude of sine wave

- M*cos(wt + phi) <=> Me^(j * theta)

- M <=> M: Amplitude of cosine wave
- theta <=> -phi: Negative amplitude of sine wave

- In complex notation, 3cos(wt + pi/4) becomes 3e^(-j*(pi / 4)) for example

-- Complex Fourier Transform

- See figure15.jpg for Discrete Fourier Transform formula (or Real DFT formula)
- See figure16.jpg and figure17.jpg for Mathematical Equivalence formulas

- See figure18.jpg for Complex DFT formula using Polar Notation
- See figure19.jpg for Complex DFT formula using Rectangular Notation

- See figure20.jpg for Inverse Complex DFT formula



-- FFT Overview:

- N points: X[0] to X[N - 1] (each part split into ReX[i] and ImX[i],
where i is the current point)

1. N points time domain signal gets decomposed into N time domain signals
each made of a single point

2. Calculate N frequency spectra corresponding to these N time domain signals

3. Synthesize N frequency spectra into a single frequency spectrum

- See figure21.jpg and figure22.jpg for a visual dissection of the decomposition

- The signals are also given in bit reversed order (see figure23.jpg)

-- Fast Fourier Transform

- See figure24.jpg for a review of the Complex DFT equation

- In FFT case, calculate Complex DFT for all points, from 0 to N - 1 and plug N in where n is


-- Digital Filter Design

- Used for Signal Separation and Signal Restoration

- Analog filters are generally better than digital filters

- Input and output signals are usually in the Time Domain

- Every Linear Filter has an Impulse Response, a Step Response,
and a Frequency Response

-- Filter kernel

- Input and Impulse Responses create an Output Response

- These can also be designed by recursion

- Recursive filters are referred to as IIRs (Infinite Impulse Responses) Filters
because the impulse responses are infinite (but values stop becoming relevant after
a certain point and can be ignored)

-- If you perform integration on Impulse Responses you get a Frequency Response

-- Logarithmic Scale and Decibels

- A bel is a power change by a factor of 10 (4 bels = 10x10x10x10 = 10000)

- decibel (dB) = 1/10th of a bel

- 0dB: 1, -10dB: 0.1, -20dB: 0.01, 10dB = 10, 20dB = 100

- dBV = 1 volt RMS signal

- -3dB = amplitude reduction of 0.707 power reduction of 0.5

-- Signal Representations of Information

- Time Domain Information

- Frequency Domain Information

-- Time Domain Parameters

- See figure25.jpg and figure26.jpg and figure27.jpg

-- Frequency Domain Parameters

- Passband: Frequencies allowed to pass
- Transition band: Frequencies between passband and stopband
- Stopband: Frequencies now blocked
- Fast roll off: Narrow transmission band

-- Filter Design Using Spectral Inversion

- Filter kernel signal values are flipped signs, and the center value greatly exaggerates (adds 1)
- Frequency response gets inverted

-- Filter Design Using Spectral Reversal

- Frequency response is reversed -- last values in frequency become first, first become last, etc
- Filter kernel signal values are flipped signs EVERY OTHER VALUE

-- See figure28.jpg for classifications of digital filters


-- Finite Impulse Response (FIR) Filters

- Moving Average Filters
  - Best for time domain signals, worst for frequency domain signals
  - See figure29.jpg for FIR Filter formula

- Recursive Moving-Average Algorithm
  - See figure30.jpg for formula

- Other Average Filter includes Multiple Pass Moving Average Filter

-- Infinite Impulse Response (IIR) Filters

- See figure31.jpg for Recursion Equation

- Other Recursive Filter includes Single Pole Recursive Filter and Digital Chebyshev filter
- When designing a Chebyshev filter, take the following steps:
  - HP or LP response
  - Cutoff frequency
  - Percentage of passband ripples
  - # of poles

-- Sinc Function/Truncated Sinc Filter

- Ideal Filter Kernel is the inverse of the Ideal Frequency Response

- Ideal Filter Kernel is given by the sinc function: sin(x) / x 
- Ideal Frequency Response is given by h[i] = sin(2*pi*fc*i) / i*pi

- Ideal Filter Kernels/Frequency Responses are conceptually infinite
and don't fit real world scenarios and are always dependent on future
samples, so the solution to that is truncated sinc filters

- Truncated Sinc Filters
  - Points truncated to M + 1 points, symetrically chosen around the lobe
  - All points outside M + 1 points are set to 0 (or ignored)
  - Entire sequence is shifted to the right so that it runs from 0 to M

- Multiplying the Truncated Sinc Filter Kernel by the Blackman/Hamming Window results
in what's called the Windowed Sinc Filter Kernel

- See figure32.jpg for the Hamming and Blackman Window equations

-- Designing a Windowed Sinc Filter

- Required parameters: Cutoff frequency (fc) and Filter Kernel length (M)

- M roughly equals 4/BW (BW = bandwidth (width of a transition band)) between 0 and 0.5

- See figure33.jpg for the formula on calculating the Windowed Sinc Filter Kernel